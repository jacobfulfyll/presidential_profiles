import codecs
import pandas as pd
from nltk.corpus import wordnet as wn
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.stem.wordnet import WordNetLemmatizer
import string

with codecs.open('speeches.csv', 'r', encoding='utf-8', errors='ignore') as fdata:
    df = pd.read_csv(fdata)

df = df[1000:2000]

lemmatizer = WordNetLemmatizer() # Create Lemmatizer

def create_dataset(df):
    inaug_df = pd.DataFrame(columns=['word', 'pos', 'lemma', 'year', 'president']) #Create empty DF
    counter = 0
    for idx, doc in df['text'].iteritems(): # For every inauguration speech
        doc = doc.replace('xa0', '')
        doc = doc.replace('(Applause)', '')
        doc = doc.replace('(applause)', '')
        doc = doc.replace('(Laughter)', '')
        doc = doc.replace('(laughter)', '')
        doc = doc.replace('(Laughter and applause)','')
        text = nltk.word_tokenize(doc) # Tokenize all words in current speech
        pos_tags = nltk.pos_tag(text) # Create Part of Speech Tag for every word
        year = int(df.loc[idx]['date'][-4:]) # Pull Year From Date Column, for new DF
        president = df.loc[idx]['president']
        print(idx, ' / ', len(df)) # Status Check
        for item in pos_tags:
            if item[0] in string.punctuation: # Remove Punctuation
                pass
            else:
                word = item[0].lower() # Seperate Word and Part of Speech into different variables
                part_of_speech = item[1] # To be placed in new DF

                # Lemmatize all possible words, adj, verbs, nouns, adverbs
                if str(item[1][0]) == 'J':
                    lemma_tag = 'a'
                    lemma = lemmatizer.lemmatize(item[0], lemma_tag)
                elif item[1][0] == 'V' or item[1][0] == 'N' or item[1][0] == 'R':
                    lemma_tag = str(item[1][0]).lower()
                    lemma = lemmatizer.lemmatize(item[0], lemma_tag)
                else:
                    lemma = item[0]

                inaug_df.loc[counter] = [word, part_of_speech, lemma, year, president] # Add row to DF
                counter += 1 # Move to the next row

    inaug_df.to_csv('rows_1000_2000.csv', index='False')


create_dataset(df)